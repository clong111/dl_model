{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer_init.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbTH1C1kM5NJ",
        "outputId": "4c581946-0cff-4402-e460-b1ecbd419ddd"
      },
      "source": [
        "!pip install zhon"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: zhon in /usr/local/lib/python3.6/dist-packages (1.1.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gftSYExLBGm"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as Data\n",
        "import random\n",
        "import time\n",
        "import string\n",
        "import zhon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1RWgkBeMO85"
      },
      "source": [
        "from zhon.hanzi import punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcQtS4chNM5-"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# torch.cuda.set_device(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LitzwN2_Nx7W"
      },
      "source": [
        "with open('train.txt', 'r') as file:\n",
        "  train = file.readlines()\n",
        "with open('test.txt', 'r') as file:\n",
        "  test = file.readlines()\n",
        "with open('dev.txt', 'r') as file:\n",
        "  dev = file.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDRZO2VLN9Fr"
      },
      "source": [
        "\n",
        "\n",
        "# 主要处理英文部分，1.有的单词标点和单词相连接，进行分离处理，2.字母小写\n",
        "# 分离句子d中的所有单词和标点\n",
        "def splitw(d):\n",
        "  \n",
        "  res = []\n",
        "  for w in d:\n",
        "    if w in string.punctuation or w == '...' or w == '..':\n",
        "      res.append(w[0])\n",
        "      continue\n",
        "    p = []\n",
        "    while w and w[0] in string.punctuation:\n",
        "      res.append(w[0])\n",
        "      w = w[1:]\n",
        "    while w and w[-1] in string.punctuation:\n",
        "      p.append(w[-1])\n",
        "      w = w[:-1]\n",
        "    if w:\n",
        "      res.append(w)\n",
        "    res.extend(p)\n",
        "\n",
        "  return res\n",
        "def datapre(data, maxen=50, rp = True):\n",
        "  data1 = [d[:-1] for d in data]\n",
        "  data2 = [d.split() for d in data1]\n",
        "  datazh = [d for i, d in enumerate(data2) if i % 2 == 0]\n",
        "  dataen = [d for i, d in enumerate(data2) if i % 2 == 1]\n",
        "  dataen1 = [splitw(d) for d in dataen] \n",
        "  dataen2 = [[w.lower() for w in d] for d in dataen1]\n",
        "\n",
        "  # 默认去掉所有标点，否则标点符号统一化\n",
        "  if rp:\n",
        "    dataen3 = [[w for w in d if w not in string.punctuation] for d in dataen2]\n",
        "    datazh1 = [[w for w in d if w not in punctuation] for d in datazh]\n",
        "  else:\n",
        "    dataen3 = [[\".\" if w in string.punctuation else w for w in d] for d in dataen2]\n",
        "    datazh1 = [[\".\" if w in punctuation else w for w in d] for d in datazh]\n",
        "\n",
        "  # 利用padding填充数据，有一些超长的句子就省略了\n",
        "  # maxen = 50\n",
        "  idx = [i for i, d in enumerate(dataen3) if len(d)<=50]\n",
        "  dataen4 = [dataen3[i] for i in idx]\n",
        "  datazh2 = [datazh1[i] for i in idx]\n",
        "  maxzh = max(len(d) for d in datazh2)\n",
        "  enpad = [d + ['<padding>'] * (maxen - len(d)) for d in dataen4]\n",
        "  zhpad = [d + ['<padding>'] * (maxzh - len(d)) for d in datazh2]\n",
        "\n",
        "  # 为句子补上起始符和终止符\n",
        "  enfinal = [['<SOS>'] + d + ['<EOS>'] for d in enpad]\n",
        "  zhfinal = [['<SOS>'] + d + ['<EOS>'] for d in zhpad]\n",
        "\n",
        "  return enfinal, zhfinal\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SM5dmTROHV4"
      },
      "source": [
        "# 将词汇数据处理为id数据\n",
        "tr_en, tr_zh = datapre(train)\n",
        "ts_en, ts_zh = datapre(test)\n",
        "enlist, zhlist = [], []\n",
        "for l in tr_en + ts_en:\n",
        "  enlist += l\n",
        "for l in tr_zh + ts_zh:\n",
        "  zhlist += l \n",
        "en_vocab_size = len(set(enlist))\n",
        "zh_vocab_size = len(set(zhlist))\n",
        "en_id2char = list(set(enlist))\n",
        "zh_id2char = list(set(zhlist))\n",
        "en_char2id = {c:i for i,c in enumerate(en_id2char)}\n",
        "zh_char2id = {c:i for i,c in enumerate(zh_id2char)}\n",
        "\n",
        "# 将<padding>的索引指向0\n",
        "eni = en_char2id['<padding>']\n",
        "zhi = zh_char2id['<padding>']\n",
        "enc = en_id2char[0]\n",
        "zhc = zh_id2char[0]\n",
        "en_char2id['<padding>'], zh_char2id['<padding>'] = 0, 0\n",
        "en_char2id[enc], zh_char2id[zhc] = eni, zhi\n",
        "en_id2char[0], zh_id2char[0] = '<padding>', '<padding>'\n",
        "en_id2char[eni], zh_id2char[zhi] = enc, zhc\n",
        "\n",
        "trid_en, tsid_en =[[en_char2id[c] for c in d] for d in tr_en], [[en_char2id[c] for c in d] for d in ts_en]\n",
        "trid_zh, tsid_zh =[[zh_char2id[c] for c in d] for d in tr_zh], [[zh_char2id[c] for c in d] for d in ts_zh]\n",
        "\n",
        "trid_en, tsid_en = torch.tensor(trid_en), torch.tensor(tsid_en)\n",
        "trid_zh, tsid_zh = torch.tensor(trid_zh), torch.tensor(tsid_zh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqL1ej3a6YRn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS5hTJDXOO_R"
      },
      "source": [
        "# 参数设计\n",
        "emb_dim = 512\n",
        "feedforward_dim = 2048\n",
        "k_dim = v_dim = 64\n",
        "num_layers = 6\n",
        "num_heads = 8\n",
        "\n",
        "maxen = len(trid_en[0])\n",
        "# maxzh代表中文句子的长度，作为解码器的输入，需要带起始标志，而输出则不需要。为了形状统一，将句子长度-1\n",
        "maxzh = len(trid_zh[0]) - 1\n",
        "dtype = torch.float32\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EseA-3kf8sEq",
        "outputId": "de1e400a-89ce-48ad-9261-bc9f243a0ba5"
      },
      "source": [
        "print(maxen, maxzh)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "52 87\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DUu48nRHFOh"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  # 玛德，忘记加下划线了\n",
        "  def __init__(self, model_dim=emb_dim, dropout=0.1, max_len=maxen):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    pe = torch.zeros(max_len, model_dim)\n",
        "    position = torch.arange(max_len, dtype = dtype).unsqueeze(1)\n",
        "    divterm = torch.exp(torch.arange(0, model_dim, 2).float() * (-math.log(10000.0) / model_dim))\n",
        "    pe[:, 0::2] = torch.sin(position * divterm)\n",
        "    pe[:, 1::2] = torch.cos(position * divterm)\n",
        "    # 这一步增加一维，满足x的形状\n",
        "    pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "    # 命名为pe,forward中的self.pe源于此,更重要的是固定tensor的值，tensor的值在之后反向传播中值不会变\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x的形状 (seq_len, batch_size, emb_dim)\n",
        "    # 这里x的形状原先是(b, s, emb)，这样一来输入调整了一次，输出又要调回去，主要是为了照顾x可能超长\n",
        "    \n",
        "    x += self.pe[:x.size(0), :]\n",
        "    return self.dropout(x)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hvc118eeT7L0",
        "outputId": "e2d48e4a-f155-4a2b-b69e-f6a3ffa3eb07"
      },
      "source": [
        "test = PositionalEncoding()\n",
        "x = torch.zeros(maxen, emb_dim).unsqueeze(1)\n",
        "test(x).shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([52, 1, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f3exV78G0BB"
      },
      "source": [
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "  batch_size, len_q = seq_q.size()\n",
        "  batch_size, len_k = seq_k.size()\n",
        "  mask = seq_k.data.eq(0).unsqueeze(1)\n",
        "  return mask.expand(batch_size, len_q, len_k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33LQ0KZfsLIf"
      },
      "source": [
        "# tmp = torch.tensor([0, 1, 0]).unsqueeze(0)\n",
        "# get_attn_pad_mask(tmp, tmp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VnVFxfwm_Ew"
      },
      "source": [
        "def get_subsequence_mask(seq):\n",
        "  # seq: [batch_size, len_seq]\n",
        "  attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
        "  mask = np.triu(np.ones(attn_shape), k=1)\n",
        "  # 用byte是为了之后能使用masked_fill\n",
        "  # bool()可以用来避免userwarning，不过之后是用来是要和其他mask相加的，可以省略\n",
        "  mask = torch.from_numpy(mask).byte()\n",
        "  return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAb8aTA4orr0"
      },
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ScaledDotProductAttention, self).__init__()\n",
        "  def forward(self, q, k, v, attn_mask):\n",
        "    # k: [batch_size, n_heads, len_k, k_dim]\n",
        "    # matmul对于多维情况，是对最后两维做矩阵乘法\n",
        "    scores = torch.matmul(q, k.transpose(-1, -2)) / np.sqrt(k_dim)\n",
        "    # 把scores中的1全部替换为-inf(-1e9)，scores要为byte类型\n",
        "    # 下划线代表修改原tensor\n",
        "    scores.masked_fill_(attn_mask, -1e9)\n",
        "    attn = nn.Softmax(dim=-1)(scores)\n",
        "    context = torch.matmul(attn, v)\n",
        "\n",
        "    return context, attn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afV-wRvzLRwE"
      },
      "source": [
        "# test = ScaledDotProductAttention()\n",
        "# tmp = torch.randn(2, 3, 4, 5)\n",
        "# sq = sk = torch.tensor([[1, 0, 0, 0],[2 , 3, 0, 0]])\n",
        "# sq.to(device)\n",
        "# sk.to(device)\n",
        "# tmp = tmp.to(device)\n",
        "# # 两种mask都测试一下\n",
        "# mask = get_attn_pad_mask(sq, sk)\n",
        "# mask = get_subsequence_mask(sq)\n",
        "# m = mask.unsqueeze(1).repeat(1, 3, 1, 1)\n",
        "# m = m.to(device)\n",
        "# c, a = test(tmp, tmp, tmp, m)\n",
        "\n",
        "# print(a.shape, c.shape)\n",
        "# print(a[0, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJBcG-7VOoJq"
      },
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self, model_dim=emb_dim):\n",
        "    super(MultiheadAttention, self).__init__()\n",
        "    # query和key的维度要相同\n",
        "    self.wq = nn.Linear(model_dim, k_dim * num_heads, bias=False)\n",
        "    self.wk = nn.Linear(model_dim, k_dim * num_heads, bias=False)\n",
        "    self.wv = nn.Linear(model_dim, v_dim * num_heads, bias=False)\n",
        "    self.dense = nn.Linear(v_dim * num_heads, model_dim, bias=False)\n",
        "    self.model_dim = model_dim\n",
        "  def forward(self,input_q, input_k, input_v, mask):\n",
        "    residual, batch_size = input_q, input_q.shape[0]\n",
        "    # 不能直接将q修改到最终的形状而需要通过transpose，因为经过self.wq后,\n",
        "    # 形状为[batch_size, len_q, num_heads * k_dim]\n",
        "    q = self.wq(input_q).view(batch_size, -1, num_heads, k_dim).transpose(1, 2)\n",
        "    # k,v的length相同，即下面-1的维度\n",
        "    k = self.wk(input_k).view(batch_size, -1, num_heads, k_dim).transpose(1, 2)\n",
        "    v = self.wv(input_v).view(batch_size, -1, num_heads, v_dim).transpose(1, 2)\n",
        "    mask = mask.unsqueeze(1).repeat(1, num_heads, 1, 1)\n",
        "    # 这里可以不用作为model，因为该module类并没有神经网络参数\n",
        "    context, attn = ScaledDotProductAttention()(q, k, v, mask)\n",
        "    # context: [batch_size, num_heads, len_q, v_dim]\n",
        "    # 将context变换，以便将其输入self.dense,最终能与residual进行加和\n",
        "    context = context.transpose(1, 2).reshape(batch_size, -1, v_dim * num_heads)\n",
        "    output = self.dense(context)\n",
        "    return nn.LayerNorm(emb_dim).cuda()(output + residual), attn\n",
        "\n",
        "# tmp = torch.randn(2, 4, emb_dim)\n",
        "# tmp = tmp.to(device)\n",
        "# mask = get_attn_pad_mask(sq, sk).to(device)\n",
        "# test = MultiheadAttention().to(device)\n",
        "# print(tmp.device)\n",
        "# iv, a = test(tmp, tmp, tmp, mask)\n",
        "# print(iv.shape, a.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlwWalbLlVyE"
      },
      "source": [
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "  def __init__(self, model_dim=emb_dim):\n",
        "    super(PoswiseFeedForwardNet, self).__init__()\n",
        "    self.dense = nn.Sequential(\n",
        "        nn.Linear(model_dim, feedforward_dim, bias=False),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(feedforward_dim, model_dim, bias=False)\n",
        "        )\n",
        "    # self.dense = self.dense.to(device)\n",
        "    self.model_dim = model_dim\n",
        "  def forward(self, inputs):\n",
        "    residual = inputs\n",
        "    outputs = self.dense(inputs)\n",
        "    return nn.LayerNorm(emb_dim).cuda()(outputs + residual)\n",
        "# test = PoswiseFeedForwardNet()\n",
        "# tmp1 = torch.randn(2, 4, emb_dim)\n",
        "# # tmp1 = tmp1.to(device)\n",
        "# test(tmp1).shape "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax2TljR-qGuI"
      },
      "source": [
        "# encoder layer，单独设一类是因为一个encoder中可以有多个encoder layer\n",
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.attention = MultiheadAttention()\n",
        "    self.feedforward = PoswiseFeedForwardNet()\n",
        "  def forward(self, inputs, mask):\n",
        "    outputs, attn = self.attention(inputs, inputs, inputs, mask)\n",
        "    outputs = self.feedforward(outputs)\n",
        "    return outputs, attn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQN5c38qpgt8"
      },
      "source": [
        "# test = EncoderLayer().to(device)\n",
        "# tmp = torch.randn(2, 4, emb_dim)\n",
        "# tmp = tmp.to(device)\n",
        "# mask = get_subsequence_mask(torch.randn(2, 4).to(device))\n",
        "# ot, a = test(tmp, mask)\n",
        "# print(ot.shape, a.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vim05-LuJHr"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Encoder, self).__init__()\n",
        "    # 这里是翻译任务，英语转中文，中英文的emb_dim相同\n",
        "    self.en_embedding = nn.Embedding(en_vocab_size, emb_dim)\n",
        "    self.pos_embedding = PositionalEncoding()\n",
        "    self.layers = nn.ModuleList([EncoderLayer() for _ in range(num_layers)])\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # inputs: [batch_size, seq_len]\n",
        "    en_embs = self.en_embedding(inputs)\n",
        "    pos_embs = self.pos_embedding(en_embs.transpose(0, 1)).transpose(0, 1)\n",
        "    mask = get_attn_pad_mask(inputs, inputs)\n",
        "    outputs, attns = pos_embs, []\n",
        "    for layer in self.layers:\n",
        "      outputs, attn = layer(outputs, mask)\n",
        "      attns.append(attn)\n",
        "    return outputs, attns\n",
        "\n",
        "# test = Encoder().to(device)\n",
        "# tmp = torch.arange(6).view(2, 3)\n",
        "# tmp = tmp.to(device)\n",
        "# ot, a = test(tmp)\n",
        "# print(ot.shape, a[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66R_bQ03unkM"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.self_attention = MultiheadAttention()\n",
        "    self.dec_enc_attention = MultiheadAttention()\n",
        "    self.feedforward = PoswiseFeedForwardNet()\n",
        "  \n",
        "  def forward(self, dec_inputs, enc_outputs, self_mask, enc_mask):\n",
        "    dec_outputs, self_attn = self.self_attention(dec_inputs, dec_inputs, dec_inputs, self_mask)\n",
        "    dec_outputs, dec_enc_attn = self.dec_enc_attention(dec_outputs, enc_outputs, enc_outputs, enc_mask)\n",
        "    dec_outputs = self.feedforward(dec_outputs)\n",
        "    return dec_outputs, self_attn, dec_enc_attn\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G0C1zcNClqO"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, model_dim=emb_dim):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.ch_embedding = nn.Embedding(zh_vocab_size, model_dim)\n",
        "    self.pos_embedding = PositionalEncoding(max_len=maxzh)\n",
        "    self.layers = nn.ModuleList([DecoderLayer() for _ in range(num_layers)])\n",
        "  \n",
        "  # 这里enc_inputs仅仅用于构造enc2dec\n",
        "  def forward(self, dec_inputs, enc_inputs, enc_outputs):\n",
        "    dec_outputs = self.ch_embedding(dec_inputs)\n",
        "    dec_outputs = self.pos_embedding(dec_outputs.transpose(0, 1)).transpose(0, 1).cuda()\n",
        "    # 至于遮挡到哪里，以\"我很好\"为例，当我提出\"很\"这个query时，我只能看到\"我很\"两个内容\n",
        "    dec_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).cuda()\n",
        "    dec_seq_mask = get_subsequence_mask(dec_inputs).cuda()\n",
        "    dec_mask = torch.gt((dec_pad_mask + dec_seq_mask), 0).cuda()\n",
        "    # enc_mask\n",
        "    enc_mask = get_attn_pad_mask(dec_inputs, enc_inputs)\n",
        "    # 这里命名其实不太准确，dec_attn代表decoder的自注意力分数，enc_attn代表decoder和encoder的联合注意力分数\n",
        "    dec_attns, enc_attns = [], []\n",
        "    for layer in self.layers:\n",
        "      dec_outputs, dec_attn, enc_attn = layer(dec_outputs, enc_outputs, dec_mask, enc_mask)\n",
        "      dec_attns.append(dec_attn)\n",
        "      enc_attns.append(enc_attn)\n",
        "    return dec_outputs, dec_attns, enc_attns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMlkScLZbZaL"
      },
      "source": [
        "# testenc = Encoder().cuda()\n",
        "# testdec = Decoder().cuda()\n",
        "# # enc = torch.arange(6).view(2, 3)\n",
        "# # dec = torch.arange(8).view(2, 4)\n",
        "\n",
        "# dec = trid_zh[:2, :]\n",
        "# enc = trid_en[:2, :]\n",
        "# enc_ot, ea = testenc(enc)\n",
        "# # enc_ot = enc_ot.to(device)\n",
        "# dec_ot, da, eda = testdec(dec, enc, enc_ot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsvmQYS4ZeWs"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, model_dim = emb_dim):\n",
        "    super(Transformer,self).__init__()\n",
        "    self.encoder = Encoder().cuda()\n",
        "    self.decoder = Decoder().cuda()\n",
        "    self.projection = nn.Linear(model_dim, zh_vocab_size, bias=False).cuda()\n",
        "  def forward(self, enc_inputs, dec_inputs):\n",
        "    enc_outputs, enc_self_attns = self.encoder(enc_inputs)\n",
        "    # enc_outputs = enc_outputs.to(device)\n",
        "    dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
        "    dec_logits = self.projection(dec_outputs)\n",
        "    \n",
        "    return dec_logits.view(-1, dec_logits.shape[-1]), enc_self_attns, dec_self_attns, dec_enc_attns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyNNuREIZ1wB"
      },
      "source": [
        "# test = Transformer().to(device)\n",
        "# enc = torch.arange(6).view(2, 3)\n",
        "# dec = torch.arange(8).view(2, 4)\n",
        "# enc = enc.to(device)\n",
        "# dec = dec.to(device)\n",
        "\n",
        "# dl, ea, da, dea = test(enc, dec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAkX0xXc8iiE"
      },
      "source": [
        "# 将预测结果以好观察的形式呈现\n",
        "def sentence(lis):\n",
        "  j = len(lis) - 1\n",
        "  while lis[j] == '<padding>' or lis[j] == '<EOS>':\n",
        "    j -= 1\n",
        "    if j == -1:\n",
        "      return ''\n",
        "  return ''.join(lis[:j+1])\n",
        "def translate(model, tri, tsi):\n",
        "  tren,tsen = trid_en[tri].unsqueeze(0), tsid_en[tsi].unsqueeze(0)\n",
        "  trzh, tszh = trid_zh[tri].unsqueeze(0), tsid_zh[tsi].unsqueeze(0)\n",
        "  tren, tsen, trzh, tszh = tren.cuda(), tsen.cuda(), trzh.cuda(), tszh.cuda()\n",
        "  tro, ea, da, eda = model(tren, trzh[:, :-1])\n",
        "  tso, ea, da, eda = model(tsen, tszh[:, :-1])\n",
        "  tro1, tso1 = tro.squeeze(1).argmax(1), tso.squeeze(1).argmax(1)\n",
        "  tro2, tso2 = [zh_id2char[i] for i in tro1], [zh_id2char[i] for i in tso1]\n",
        "  print('train sentence:', sentence(tro2))\n",
        "  print('test sentence:', sentence(tso2))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0GHhvEAIXRJ"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8_AnQWWqLZJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3960bd46-1769-4340-ab21-5058d3cf32d7"
      },
      "source": [
        "# 开始训练\n",
        "num_epochs = 50\n",
        "batch_size = 128\n",
        "loss = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "# 注，这里emb_dim编码器解码器共用\n",
        "\n",
        "model = Transformer().cuda()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=3e-3, momentum=0.99)\n",
        "tri, tsi = 10, 10\n",
        "print('correct train', sentence(tr_zh[tri]))\n",
        "print('correct test', sentence(ts_zh[tsi]))\n",
        "for epoch in range(num_epochs):\n",
        "  start, loss_sum, n = time.time(), 0.0, 0\n",
        "  dataset = torch.utils.data.TensorDataset(trid_en, trid_zh)\n",
        "  train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)\n",
        "  for tren, trzh in train_iter:\n",
        "    tren, trzh = tren.cuda(), trzh.cuda()\n",
        "    enc_inputs = tren\n",
        "    dec_inputs = trzh[:, :-1]\n",
        "    dec_outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n",
        "    # dec_inputs的ground truth相对于标签是要左移一位的，\n",
        "    # 相当于dec_inputs \"<SOS>我很好\" 对应的表签y \"我很好<EOS>\"\n",
        "    y = trzh[:, 1:]\n",
        "    # 对比sq2sq，模型输出的结果是逐个时间步产生的，因此需要transpose，\n",
        "    # 而transformer只是在self attention是逐个时间步计算，最终输出是全序列一次性输出，所以不用transpose\n",
        "    yt = y.contiguous().view(-1)\n",
        "    l = loss(dec_outputs, yt.long())\n",
        "    # 训练三部曲\n",
        "    optimizer.zero_grad()\n",
        "    l.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_sum += l.item() * trzh.shape[0]\n",
        "    n += trzh.shape[0]\n",
        "  try:\n",
        "    perplexity = math.exp(loss_sum / n)\n",
        "  except OverflowError:\n",
        "    perplexity = float('inf')\n",
        "  print(\"epoch:{} perplexity:{} time:{}\".format(epoch, perplexity, time.time() - start))\n",
        "  translate(model, tri, tsi)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "correct train <SOS>播用跑道号码更是幼稚得可怜用5左5右这种令人混乱的数字为什么不采用5678等等\n",
            "correct test <SOS>在西伯伦市中心的犹太人定居点外巴勒斯坦示威者与警察发生了冲突\n",
            "epoch:0 perplexity:7792.580768721553 time:61.98250985145569\n",
            "train sentence: 是是是是<EOS>是<EOS>是是是是<EOS>是<EOS><EOS>一的<EOS>的<EOS>是<EOS><EOS>是\n",
            "test sentence: 是是是是是是是是一<EOS><EOS><EOS>是是\n",
            "epoch:1 perplexity:3580.3433243222144 time:61.498366594314575\n",
            "train sentence: 是是是是的是的的的的的的的的的的的的的了的的的的的的的的的\n",
            "test sentence: 是是是是是的的的的的的的的的的\n",
            "epoch:2 perplexity:2656.17660833955 time:61.20496463775635\n",
            "train sentence: 的的的的的的的的的的的的的的的的的的的的的的的的的的的的的\n",
            "test sentence: 的的的的的的的的的的的的的的的\n",
            "epoch:3 perplexity:2278.9264785740397 time:61.18912220001221\n",
            "train sentence: 他的的的的的的的的的的的的的的的的的的的的的的的的的的的的\n",
            "test sentence: 他的的的的的的的的的的的的的的\n",
            "epoch:4 perplexity:2068.2278818874634 time:61.16396689414978\n",
            "train sentence: 在在在在的的的的的的的的的的的的的的的一的的的的的的的的的\n",
            "test sentence: 在在在的一的的的的的的的的的的\n",
            "epoch:5 perplexity:1926.0342222963113 time:61.200403928756714\n",
            "train sentence: 在在在在的的的的的的的的的的的的的的的一的的的的的的的的的\n",
            "test sentence: 在这在的一的的的的的的的的的的\n",
            "epoch:6 perplexity:1822.4820062942374 time:61.220499992370605\n",
            "train sentence: 他在的的的的的的的的的的的的的的的的的一的的的的的的的的的\n",
            "test sentence: 他他的的一的的的的的的的的一的\n",
            "epoch:7 perplexity:1738.09183986421 time:61.15712904930115\n",
            "train sentence: 这在的的的一的的的的的的的的一的的的的一的的的的的的的的的\n",
            "test sentence: 这这的的一的的的的的的的的一的\n",
            "epoch:8 perplexity:1654.6541055684124 time:61.20753836631775\n",
            "train sentence: 在在的的的一的的的的的的的的一的的的的一的的是的的的的的的\n",
            "test sentence: 在美国的的一的的的的的的的的一的\n",
            "epoch:9 perplexity:1565.3084368629459 time:61.250619411468506\n",
            "train sentence: 在在在在在一在的的的的的的的一的的的的一的的会的的的的的的\n",
            "test sentence: 在美国的的一的的的的的的的的一在\n",
            "epoch:10 perplexity:1479.1011869921967 time:61.22444438934326\n",
            "train sentence: 他说的说说一的的的的的的的的一的的的的人的的会的的的的的的\n",
            "test sentence: 他这的的人的的的的的的的了一的\n",
            "epoch:11 perplexity:1392.9922773989217 time:61.24154472351074\n",
            "train sentence: 在说的在说一的的的的的的的的次的的的的一的的会的的的的的的\n",
            "test sentence: 在中国的的是和的的的的的的了一的\n",
            "epoch:12 perplexity:1308.6508750316689 time:61.19978046417236\n",
            "train sentence: 在这是一说一是的的的的的的的一的的在的一一的会的的的的的的\n",
            "test sentence: 在美国的的一和的的的了美国的了一的\n",
            "epoch:13 perplexity:1225.7312481079719 time:61.31242108345032\n",
            "train sentence: 新华社说是的是在的的的的的的的的一的的的的一的的会的的的的的的\n",
            "test sentence: 他这的的一和的的的的美国的了一的\n",
            "epoch:14 perplexity:1140.476081333568 time:61.267282009124756\n",
            "train sentence: 在说是的是一的的的的的的的的一的人的的一的的是的的的的的是\n",
            "test sentence: 他一的的一和的的的了中国也了一的\n",
            "epoch:15 perplexity:1066.5273023935285 time:61.31036138534546\n",
            "train sentence: 他说在在是一的的的的个的个的一的人的的人人不能的个的的个的\n",
            "test sentence: 他美国的的一和的的的了美国也了一中\n",
            "epoch:16 perplexity:1005.7220790264529 time:61.29742622375488\n",
            "train sentence: 在说是的是一的在的的个的个的一了人都的人人和会的个了的个是\n",
            "test sentence: 在这的的一和的的人民了中国也了一中\n",
            "epoch:17 perplexity:932.0739017173441 time:61.285637855529785\n",
            "train sentence: 他说在他多在的的的的个的的的一的人的的一的不是的的的的的的\n",
            "test sentence: 他上的的一中的的的了美国的了一中\n",
            "epoch:18 perplexity:865.9065390274017 time:61.29052543640137\n",
            "train sentence: 在说的他多在的的的的个的个的一一人在的一的不会的个的的个的\n",
            "test sentence: 在这的的一中的的的了美国的了一中\n",
            "epoch:19 perplexity:802.0997722142295 time:61.30488848686218\n",
            "train sentence: 在说的的多在的的的的的的个的一的人的的人的不是的个的的个的\n",
            "test sentence: 在这的中是中的的的了美国的的一中\n",
            "epoch:20 perplexity:739.8207789350107 time:61.28686475753784\n",
            "train sentence: 在说在他多一的的的的个的个对次一人受伤了人人不会的个个的个的\n",
            "test sentence: 在这的中报道和中的人民了美国中了一中\n",
            "epoch:21 perplexity:679.9859530787672 time:61.285999059677124\n",
            "train sentence: 他说的他多一的的的的个的个的一的人的的人人不是的个的名个的\n",
            "test sentence: 在这的中报道和的的的了的的了一中\n",
            "epoch:22 perplexity:627.0036514340416 time:61.26243877410889\n",
            "train sentence: 他说说他多在我在的我个的个的一一人在一一人不是的个个名个的\n",
            "test sentence: 在这的中一和的的人民在美国的的一中\n",
            "epoch:23 perplexity:574.651414146094 time:61.32620334625244\n",
            "train sentence: 在说是说多一的一的的个的个的一一人的一人人不是人个个名人的\n",
            "test sentence: 在中国的上经济和的的的的美国的的一的\n",
            "epoch:24 perplexity:526.4051443735716 time:61.28871011734009\n",
            "train sentence: 在说说说多在在的的的人就个的次两人被在人人不会的个个名人都\n",
            "test sentence: 在上的的经济和的公司的在美国的的两的\n",
            "epoch:25 perplexity:484.71838944734463 time:61.29951047897339\n",
            "train sentence: 在说是说多一在的的的个的人的次一人受伤在人人不是这个个个年都\n",
            "test sentence: 在上的中报道和中的人民了美国也了这的\n",
            "epoch:26 perplexity:445.7499682099963 time:61.2308566570282\n",
            "train sentence: 在说是一是在在不的的个的个的一事情人在在人人不是人个个的人的\n",
            "test sentence: 在这的中中国和的公司的了美国的了这的\n",
            "epoch:27 perplexity:410.31190945550173 time:61.295732498168945\n",
            "train sentence: 这说是说多在的不的的人的个的一一人的的一有不会的个个的名的\n",
            "test sentence: 在上的的经济和的的的了美国的的\"的\n",
            "epoch:28 perplexity:376.1225620413244 time:61.302276372909546\n",
            "train sentence: 在的上一多一的的的的人的人的一一人死亡在人有不是的人个名人的\n",
            "test sentence: 在美国的中发展和的的的的美国的的一的\n",
            "epoch:29 perplexity:343.2832704137328 time:61.26350998878479\n",
            "train sentence: 这说被有多在的不的的人的人的次事情人的有人有不能人人个名人的\n",
            "test sentence: 在上的上候选人中的的的了美国的了一中\n",
            "epoch:30 perplexity:313.85991421651215 time:61.30859565734863\n",
            "train sentence: 据说是说是在我对的这个人的的的次两人受伤的人安全不会的个个的人的\n",
            "test sentence: 在中国的上经济和的的的了美国的的他们中\n",
            "epoch:31 perplexity:287.4689699216175 time:61.27080464363098\n",
            "train sentence: 这说上有多在在一的一个名个的一一人死亡的人有不是的个个名人都\n",
            "test sentence: 在上上上问题上中的方面对其他的的他们中\n",
            "epoch:32 perplexity:262.7645851975371 time:61.32606816291809\n",
            "train sentence: 这有的说是因为的不的的个的个的一事情人都有人人不是的人个名人的\n",
            "test sentence: 在上的上经济和的的的对其他的的新的\n",
            "epoch:33 perplexity:241.90753858617012 time:61.26578950881958\n",
            "train sentence: 这有的说多在的很的的天的个的种事情人都的钱有不会的个个个个都\n",
            "test sentence: 在这期间上候选人和的的的了其他的的一的\n",
            "epoch:34 perplexity:220.99272414286102 time:61.28872323036194\n",
            "train sentence: 这这上这多在的很在这个个5个的一事情人的的人人不能这个个个人都\n",
            "test sentence: 在这方面上支持和中的的了其他在了他们的\n",
            "epoch:35 perplexity:201.8256510019657 time:61.2613959312439\n",
            "train sentence: 这有是表示多一在不的这个个5的的一事情人都的人有不会他们个个个的都\n",
            "test sentence: 警方以色列的中政治和中他们的对其他的的他们的\n",
            "epoch:36 perplexity:184.5965535427154 time:61.30923318862915\n",
            "train sentence: 这说上说多一不很的的的的个的种事情人都的人有不是很的个名人都\n",
            "test sentence: 警方以色列方面上巴勒斯坦上中方面方面还巴勒斯坦的的对的\n",
            "epoch:37 perplexity:168.87535532197307 time:61.28785967826843\n",
            "train sentence: 目前有的说是在的很的我个5天这种事情人死亡的人有不是还个个名人都\n",
            "test sentence: 警方以色列上上警察和和警方的对巴勒斯坦在了不少的\n",
            "epoch:38 perplexity:155.26312696896923 time:61.26218032836914\n",
            "train sentence: 而有是一是一不很还的个5个的种事情人死亡的人再不是的个个个人的\n",
            "test sentence: 警方上方面上警察和和和的还巴勒斯坦也了他们的\n",
            "epoch:39 perplexity:141.35540615958465 time:61.29516243934631\n",
            "train sentence: 目前一在给多在不更的的个5个的种事情人受伤的人再不能还人个人人都\n",
            "test sentence: 以色列以色列方面上巴勒斯坦和和方面方面还巴勒斯坦和了冲突的\n",
            "epoch:40 perplexity:128.591671641393 time:61.305771827697754\n",
            "train sentence: 而有在更多为了我很还我号5号的种事情人受伤的人人不是很个个人人都\n",
            "test sentence: 警方以色列方面上以色列和和和的还巴勒斯坦在了他们的\n",
            "epoch:41 perplexity:117.21354432220056 time:61.275707960128784\n",
            "train sentence: 这被上更是因为与很还的的5号的种事情人死亡的这安全不是5号个时所都\n",
            "test sentence: 警方上上上以色列和和警方的还以色列和的这的\n",
            "epoch:42 perplexity:106.80417558735954 time:61.26748728752136\n",
            "train sentence: 这有上这多在的很在这个号5天这种事情人死亡的人把不会5个个个人都\n",
            "test sentence: 警方以色列方面上以色列和和开始的还巴勒斯坦和了这的\n",
            "epoch:43 perplexity:97.19599781075627 time:61.294005393981934\n",
            "train sentence: 这的有有是不不很用的号5号的种事情人死亡的人有不再很个人名人已经\n",
            "test sentence: 以色列以色列上上以色列和上方面方面还巴勒斯坦在了他们的\n",
            "epoch:44 perplexity:89.1650641798594 time:61.30937433242798\n",
            "train sentence: 目前的有更是在不很还一号5号这种被人死亡的一有不是5个个时人都\n",
            "test sentence: 以色列以色列方面上以色列和和开始的还巴勒斯坦和的他们已经\n",
            "epoch:45 perplexity:81.42943190571282 time:61.278963565826416\n",
            "train sentence: 有的更这是不不很还这个个5分这种事情人死亡的这有不是5个个位名都\n",
            "test sentence: 警方以色列上上以色列警方对的的还以色列在了冲突的\n",
            "epoch:46 perplexity:74.37692962802791 time:61.292370080947876\n",
            "train sentence: 这些的上更是由于不更还这个分5分这种事情人死亡的一有不是5元人人人还\n",
            "test sentence: 以色列以色列方面上以色列和和方面和对和和的冲突的\n",
            "epoch:47 perplexity:68.07504198967939 time:61.29262161254883\n",
            "train sentence: 这的上更是这不更还这个个5个这种事情人死亡的情况造成不是5个个时元的\n",
            "test sentence: 巴勒斯坦以色列方面上巴勒斯坦和和开始方面还巴勒斯坦在了他们的\n",
            "epoch:48 perplexity:62.17759772319541 time:61.226810455322266\n",
            "train sentence: 这的在这是在的很还这比5结果这种事情人的的5再不是还的个位人还\n",
            "test sentence: 以色列以色列方面上以色列上和的方面还以色列的的巴勒斯坦以色列\n",
            "epoch:49 perplexity:57.196558093559744 time:61.29553294181824\n",
            "train sentence: 这法国上这是在不很在这个号5分这种事情人死亡的人再不是5个人位人都\n",
            "test sentence: 以色列以色列方面方面警察警方和警察的还巴勒斯坦和了巴勒斯坦的\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz-xU6WUMuMy"
      },
      "source": [
        "sentences = [\n",
        "        # enc_input           dec_input         dec_output\n",
        "        ['ich mochte ein bier P', 'S i want a beer .', 'i want a beer . E'],\n",
        "        ['ich mochte ein cola P', 'S i want a coke .', 'i want a coke . E']\n",
        "]\n",
        "\n",
        "# Padding Should be Zero\n",
        "src_vocab = {'P' : 0, 'ich' : 1, 'mochte' : 2, 'ein' : 3, 'bier' : 4, 'cola' : 5}\n",
        "src_vocab_size = len(src_vocab)\n",
        "\n",
        "tgt_vocab = {'P' : 0, 'i' : 1, 'want' : 2, 'a' : 3, 'beer' : 4, 'coke' : 5, 'S' : 6, 'E' : 7, '.' : 8}\n",
        "idx2word = {i: w for i, w in enumerate(tgt_vocab)}\n",
        "tgt_vocab_size = len(tgt_vocab)\n",
        "\n",
        "src_len = 5 # enc_input max sequence length\n",
        "tgt_len = 6 # dec_input(=dec_output) max sequence length\n",
        "\n",
        "def make_data(sentences):\n",
        "    enc_inputs, dec_inputs, dec_outputs = [], [], []\n",
        "    for i in range(len(sentences)):\n",
        "      enc_input = [[src_vocab[n] for n in sentences[i][0].split()]] # [[1, 2, 3, 4, 0], [1, 2, 3, 5, 0]]\n",
        "      dec_input = [[tgt_vocab[n] for n in sentences[i][1].split()]] # [[6, 1, 2, 3, 4, 8], [6, 1, 2, 3, 5, 8]]\n",
        "      dec_output = [[tgt_vocab[n] for n in sentences[i][2].split()]] # [[1, 2, 3, 4, 8, 7], [1, 2, 3, 5, 8, 7]]\n",
        "\n",
        "      enc_inputs.extend(enc_input)\n",
        "      dec_inputs.extend(dec_input)\n",
        "      dec_outputs.extend(dec_output)\n",
        "\n",
        "    return torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)\n",
        "\n",
        "enc_inputs, dec_inputs, dec_outputs = make_data(sentences)\n",
        "\n",
        "class MyDataSet(Data.Dataset):\n",
        "  def __init__(self, enc_inputs, dec_inputs, dec_outputs):\n",
        "    super(MyDataSet, self).__init__()\n",
        "    self.enc_inputs = enc_inputs\n",
        "    self.dec_inputs = dec_inputs\n",
        "    self.dec_outputs = dec_outputs\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.enc_inputs.shape[0]\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]\n",
        "\n",
        "loader = Data.DataLoader(MyDataSet(enc_inputs, dec_inputs, dec_outputs), 2, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDr2pICONFRx",
        "outputId": "76aeacbd-e97a-470f-ce8d-f6bef5a10e53"
      },
      "source": [
        "model = Transformer().cuda()\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.99)\n",
        "for epoch in range(30):\n",
        "    for enc_inputs, dec_inputs, dec_outputs in loader:\n",
        "      '''\n",
        "      enc_inputs: [batch_size, src_len]\n",
        "      dec_inputs: [batch_size, tgt_len]\n",
        "      dec_outputs: [batch_size, tgt_len]\n",
        "      '''\n",
        "      enc_inputs, dec_inputs, dec_outputs = enc_inputs.cuda(), dec_inputs.cuda(), dec_outputs.cuda()\n",
        "      # outputs: [batch_size * tgt_len, tgt_vocab_size]\n",
        "      outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n",
        "      loss = criterion(outputs, dec_outputs.view(-1))\n",
        "      print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 loss = 9.980536\n",
            "Epoch: 0002 loss = 9.589383\n",
            "Epoch: 0003 loss = 9.100897\n",
            "Epoch: 0004 loss = 8.223795\n",
            "Epoch: 0005 loss = 7.306498\n",
            "Epoch: 0006 loss = 6.316115\n",
            "Epoch: 0007 loss = 5.424812\n",
            "Epoch: 0008 loss = 4.721016\n",
            "Epoch: 0009 loss = 4.059168\n",
            "Epoch: 0010 loss = 3.496286\n",
            "Epoch: 0011 loss = 2.962638\n",
            "Epoch: 0012 loss = 2.541767\n",
            "Epoch: 0013 loss = 2.162373\n",
            "Epoch: 0014 loss = 1.815699\n",
            "Epoch: 0015 loss = 1.551307\n",
            "Epoch: 0016 loss = 1.381725\n",
            "Epoch: 0017 loss = 1.218789\n",
            "Epoch: 0018 loss = 1.108609\n",
            "Epoch: 0019 loss = 0.966831\n",
            "Epoch: 0020 loss = 0.831889\n",
            "Epoch: 0021 loss = 0.679625\n",
            "Epoch: 0022 loss = 0.523888\n",
            "Epoch: 0023 loss = 0.438625\n",
            "Epoch: 0024 loss = 0.397825\n",
            "Epoch: 0025 loss = 0.374220\n",
            "Epoch: 0026 loss = 0.368728\n",
            "Epoch: 0027 loss = 0.351252\n",
            "Epoch: 0028 loss = 0.285497\n",
            "Epoch: 0029 loss = 0.208163\n",
            "Epoch: 0030 loss = 0.143371\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQlAh1wRQLem"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}