{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sq2sq_attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qttibm76uR3p",
        "outputId": "7fc2cd65-696e-48fd-923e-e3793d834fdc"
      },
      "source": [
        "!pip install zhon"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: zhon in /usr/local/lib/python3.6/dist-packages (1.1.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9nWxBiilCk8"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import string\n",
        "import zhon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9eaotv6vjzR"
      },
      "source": [
        "from zhon.hanzi import punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sYRdLWWHFkr"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4oXS3oFRijp",
        "outputId": "5326d94e-2925-4879-c6b5-40790ada1736"
      },
      "source": [
        "# 实验各种东西\n",
        "a = torch.arange(24).view(2, 3, 4)\n",
        "b = a.transpose(0, 1)\n",
        "c = torch.arange(12).view(3, 4)\n",
        "embf = nn.Embedding(12, 10)\n",
        "d = embf(c)\n",
        "d1 = d.transpose(0, 1)\n",
        "# print(d.shape, d1.shape)\n",
        "\n",
        "gru = nn.GRU(10, 7, bidirectional = True)\n",
        "x, y = gru(d)\n",
        "print(x.shape, y.shape)\n",
        "\n",
        "x = c.unsqueeze(0)\n",
        "y = c.unsqueeze(1)\n",
        "z = c.unsqueeze(2)\n",
        "print(x.shape, y.shape, z.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 4, 14]) torch.Size([2, 4, 7])\n",
            "torch.Size([1, 3, 4]) torch.Size([3, 1, 4]) torch.Size([3, 4, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JmfqC6klJkq"
      },
      "source": [
        "with open('train.txt', 'r') as file:\n",
        "  train = file.readlines()\n",
        "with open('test.txt', 'r') as file:\n",
        "  test = file.readlines()\n",
        "with open('dev.txt', 'r') as file:\n",
        "  dev = file.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An0FEtjYnf_j"
      },
      "source": [
        "\n",
        "\n",
        "# 主要处理英文部分，1.有的单词标点和单词相连接，进行分离处理，2.字母小写\n",
        "# 分离句子d中的所有单词和标点\n",
        "def splitw(d):\n",
        "  \n",
        "  res = []\n",
        "  for w in d:\n",
        "    if w in string.punctuation or w == '...' or w == '..':\n",
        "      res.append(w[0])\n",
        "      continue\n",
        "    p = []\n",
        "    while w and w[0] in string.punctuation:\n",
        "      res.append(w[0])\n",
        "      w = w[1:]\n",
        "    while w and w[-1] in string.punctuation:\n",
        "      p.append(w[-1])\n",
        "      w = w[:-1]\n",
        "    if w:\n",
        "      res.append(w)\n",
        "    res.extend(p)\n",
        "\n",
        "  return res\n",
        "def datapre(data, maxen=50, rp=True):\n",
        "  data1 = [d[:-1] for d in data]\n",
        "  data2 = [d.split() for d in data1]\n",
        "  datazh = [d for i, d in enumerate(data2) if i % 2 == 0]\n",
        "  dataen = [d for i, d in enumerate(data2) if i % 2 == 1]\n",
        "  dataen1 = [splitw(d) for d in dataen] \n",
        "  dataen2 = [[w.lower() for w in d] for d in dataen1]\n",
        "\n",
        "  # 默认去掉所有标点，否则标点符号统一化\n",
        "  if rp:\n",
        "    dataen3 = [[w for w in d if w not in string.punctuation] for d in dataen2]\n",
        "    datazh1 = [[w for w in d if w not in punctuation] for d in datazh]\n",
        "  else:\n",
        "    dataen3 = [[\".\" if w in string.punctuation else w for w in d] for d in dataen2]\n",
        "    datazh1 = [[\".\" if w in punctuation else w for w in d] for d in datazh]\n",
        "\n",
        "  # 利用padding填充数据，有一些超长的句子就省略了\n",
        "  # maxen = 50\n",
        "  idx = [i for i, d in enumerate(dataen3) if len(d)<=50]\n",
        "  dataen4 = [dataen3[i] for i in idx]\n",
        "  datazh2 = [datazh1[i] for i in idx]\n",
        "  maxzh = max(len(d) for d in datazh2)\n",
        "  enpad = [d + ['<padding>'] * (maxen - len(d)) for d in dataen4]\n",
        "  zhpad = [d + ['<padding>'] * (maxzh - len(d)) for d in datazh2]\n",
        "\n",
        "  # 为句子补上起始符和终止符\n",
        "  enfinal = [['<SOS>'] + d + ['<EOS>'] for d in enpad]\n",
        "  zhfinal = [['<SOS>'] + d + ['<EOS>'] for d in zhpad]\n",
        "\n",
        "  return enfinal, zhfinal\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpBprSiM_LTu"
      },
      "source": [
        "# 将词汇数据处理为id数据\n",
        "tr_en, tr_zh = datapre(train)\n",
        "ts_en, ts_zh = datapre(test)\n",
        "enlist, zhlist = [], []\n",
        "for l in tr_en + ts_en:\n",
        "  enlist += l\n",
        "for l in tr_zh + ts_zh:\n",
        "  zhlist += l \n",
        "en_vocab_size = len(set(enlist))\n",
        "zh_vocab_size = len(set(zhlist))\n",
        "en_id2char = list(set(enlist))\n",
        "zh_id2char = list(set(zhlist))\n",
        "en_char2id = {c:i for i,c in enumerate(en_id2char)}\n",
        "zh_char2id = {c:i for i,c in enumerate(zh_id2char)}\n",
        "\n",
        "trid_en, tsid_en =[[en_char2id[c] for c in d] for d in tr_en], [[en_char2id[c] for c in d] for d in ts_en]\n",
        "trid_zh, tsid_zh =[[zh_char2id[c] for c in d] for d in tr_zh], [[zh_char2id[c] for c in d] for d in ts_zh]\n",
        "\n",
        "trid_en, tsid_en = torch.tensor(trid_en, device=device), torch.tensor(tsid_en, device=device)\n",
        "trid_zh, tsid_zh = torch.tensor(trid_zh, device=device), torch.tensor(tsid_zh, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kH0W4qjDTP2"
      },
      "source": [
        "# 编码器\n",
        "# 之前自己实现的sq2sq在如何把inputs处理为(num_steps, batch_size, n_class)时是直接用独热函数将(batch_size, num_steps)转换\n",
        "# 但其实用transpose(0, 1)更加优雅方便\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "    super().__init__()\n",
        "    # input_dim可以理解为词汇表长度\n",
        "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "    self.gru = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "    self.dense = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self, inputs):\n",
        "    # inputs的形状：(batch_size, num_steps)\n",
        "    # embed的形状：(num_steps, batch_size, emb_dim)\n",
        "    embed = self.dropout(self.embedding(inputs)).transpose(0, 1)\n",
        "    # state.shape[0] = 1*2，代表单层双向最终输出的隐状态。state.shape[1] = batchsize, state.shape[2] = enc_hid_dim\n",
        "    outputs, state = self.gru(embed)\n",
        "    # 最终将双层state横向堆叠后输入全输出层来调整形状，不要忘记用tanh归一最后结果\n",
        "    final_state = torch.tanh(self.dense(torch.cat((state[0, :, :], state[1, :, :]), dim=1)))\n",
        "    return outputs, final_state\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxHlFF2ldTyL",
        "outputId": "6742bd30-bb62-4917-978f-073f9d860859"
      },
      "source": [
        "# 测试一下encoder\n",
        "print(c.shape)\n",
        "c = c.long()\n",
        "testenc = Encoder(12, 7, 11, 17, 0.2)\n",
        "x, y = testenc(c)\n",
        "print(x.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 4])\n",
            "torch.Size([4, 3, 22]) torch.Size([3, 17])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2eKnlA-dbTt"
      },
      "source": [
        "# attention机制\n",
        "# 现在已经有编码器输出enc_outputs(num_steps, batch_size, enc_hid_dim * 2),\n",
        "# 以及decoder的单层双向隐状态state(batch_size, dec_hid_dim)。现在我们要考虑如何利用他们设计注意力机制\n",
        "# 现在已知s(t-1)，我们要求s(t-1)与enc_outputs中的每个时间步h(j)的关联性atj，进而决定c(t)=sum(att(s(t-1),h(j)))\n",
        "# 这里采用加和的形式来计算这种关联性，其实我更倾向用加权点乘，不过这样写也可以熟悉一下pytorch的语法比如unsequeeze\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "    super().__init__()\n",
        "    # 不能一步到位，要考虑vt，但我觉得vt只是让注意力机制的计算更加复杂了一点，直接一步到位也可以\n",
        "    self.attn = nn.Linear(enc_hid_dim * 2 + dec_hid_dim, dec_hid_dim, bias=False)\n",
        "    self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
        "  def forward(self, s, enc_outputs):\n",
        "    # s是解码器单步隐状态s(t-1)，enc_outputs是编码器所有的隐状态\n",
        "    num_steps = enc_outputs.shape[0]\n",
        "    batch_size = enc_outputs.shape[1]\n",
        "    # 把s从(batch_size,dec_hid_dim)扩展成(batch_size,num_steps,dec_hid_dim)，方便和编码器的每个隐状态做计算\n",
        "    srepeat = s.unsqueeze(1).repeat(1, num_steps, 1)\n",
        "    # 转置encoutputs与srepeat匹配\n",
        "    enco_t = enc_outputs.transpose(0, 1)\n",
        "    # 计算e(tj)，注意用tanh激活函数进行归一\n",
        "    et = torch.tanh(self.attn(torch.cat((enco_t, srepeat), dim=2)))\n",
        "    # self.v(et)的形状为(batch_size, num_steps, 1)，需要将最后一维删去\n",
        "    attention = self.v(et).squeeze(2)\n",
        "    return nn.functional.softmax(attention, dim=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCLrzZZou6YI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ba08ee4-9fce-49ec-a32c-ccc6a69666f6"
      },
      "source": [
        "# 测试一下attention\n",
        "print(c.shape)\n",
        "c = c.long()\n",
        "testenc = Encoder(12, 7, 11, 17, 0.2)\n",
        "enc_outputs, es = testenc(c)\n",
        "# 用es模拟一下解码器s，维度都一样\n",
        "testatt = Attention(11, 17)\n",
        "x = testatt(es, enc_outputs)\n",
        "print(x.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 4])\n",
            "torch.Size([3, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETzfc6vkyVML"
      },
      "source": [
        "# 解码器decoder\n",
        "# 解码器和编码器结构有很大的不同，最主要的是不能将全部时间步num_steps喂给解码器，每个时间步的输入依赖上个时间步输出的隐状态\n",
        "# 上个时间步的隐状态和encoder进行注意力处理后与decoder的上一个单词embedding合并，作为当前时间步的输入\n",
        "# 每个时间步最终的输出为gru的输出，单词embedding以及注意力隐状态合并后采用全连接层处理\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "    super().__init__()\n",
        "    self.output_dim = output_dim\n",
        "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "    # enc_hid_dim*2代表的是注意力机制处理后的上文信息c\n",
        "    self.gru = nn.GRU(enc_hid_dim * 2 + emb_dim, dec_hid_dim)\n",
        "    self.attention = attention\n",
        "    self.dense = nn.Linear(enc_hid_dim * 2 + dec_hid_dim + emb_dim, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self, dec_input, s, enc_outputs):\n",
        "    # dec_input:(batch_size), s:(batch_size, dec_hid_dim), enc_outputs:(num_steps, batch_size, enc_hid_dim*2)\n",
        "    # 先将enc_outputs通过attention机制处理为上文状态，为了相乘方便，对a进行扩维处理unsqueeze，对enc_outputs转置\n",
        "    a = self.attention(s, enc_outputs).unsqueeze(1)\n",
        "    enco_t = enc_outputs.transpose(0, 1)\n",
        "    # c:(1, batch_size enc_hid_dim*2)，处理成该形状是为了符合gru输入形状\n",
        "    c = torch.bmm(a, enco_t).transpose(0, 1)\n",
        "    # 将dec_input进行embedding处理，最终形状(1, batch_size, emb_dim)也是为了满足gru输入\n",
        "    dec_input = dec_input.unsqueeze(1)\n",
        "    embed = self.dropout(self.embedding(dec_input)).transpose(0, 1)\n",
        "    # gru的输入为当前步dec_input的embedding和上文信息c的合并\n",
        "    gru_input = torch.cat((c, embed), dim=2)\n",
        "\n",
        "    gru_output, gru_hid = self.gru(gru_input, s.unsqueeze(0))\n",
        "\n",
        "    # 将c, gru_output, embed合并，通过全连接层实现最后的输出\n",
        "    c = c.squeeze(0)\n",
        "    gru_output = gru_output.squeeze(0)\n",
        "    embed = embed.squeeze(0)\n",
        "    output = self.dense(torch.cat((c, gru_output, embed), dim=1))\n",
        "\n",
        "\n",
        "    # gru_hid调整形状后作为下一个时间步的输入s\n",
        "    return output, gru_hid.squeeze(0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcmsenBgDDoy",
        "outputId": "b2671742-f9dd-4084-e128-b51d226cbad4"
      },
      "source": [
        "# 测试一下decoder\n",
        "print(c.shape)\n",
        "c = c.long()\n",
        "testenc = Encoder(12, 7, 11, 17, 0.2)\n",
        "enc_outputs, es = testenc(c)\n",
        "# 用es模拟一下解码器s，维度都一样\n",
        "testatt = Attention(11, 17)\n",
        "testdec = Decoder(19, 7, 11, 17, 0.2, testatt)\n",
        "# batch_size:c.shape[0]\n",
        "dec_input = torch.arange(c.shape[0])\n",
        "dec_output, ds = testdec(dec_input, es, enc_outputs)\n",
        "print(dec_output.shape, ds.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 4])\n",
            "torch.Size([3, 19]) torch.Size([3, 17])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9MyqrelTxz9"
      },
      "source": [
        "# sq2sq模型\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self, data_en, data_zh, teacher_rate=0.5):\n",
        "    batch_size = data_en.shape[0]\n",
        "    num_steps = data_zh.shape[1]\n",
        "    # 这里没有将最开始的起始符放入outputs，所以最后的输出和中文字符全长105相比少1\n",
        "    outputs = []\n",
        "\n",
        "    enc_outputs, s = self.encoder(data_en)\n",
        "    dec_input = data_zh[:,0]\n",
        "\n",
        "    for i in range(1, num_steps):\n",
        "      dec_output, s = self.decoder(dec_input, s, enc_outputs)\n",
        "      dec_input = data_zh[:,i] if random.random() < teacher_rate else dec_output.argmax(1)\n",
        "      outputs.append(dec_output)\n",
        "    return torch.stack(outputs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwOy6Zt6Lr3j",
        "outputId": "db4129d8-b457-4e5a-b002-79a832b9e8cc"
      },
      "source": [
        "# 测试一下seq2seq\n",
        "x, y = trid_en[:6], trid_zh[:6]\n",
        "emb_dim = 256\n",
        "enc_hid_dim = 512\n",
        "dec_hid_dim = 512\n",
        "dropout = 0.5\n",
        "attention = Attention(enc_hid_dim, dec_hid_dim)\n",
        "encoder = Encoder(en_vocab_size, emb_dim, enc_hid_dim, dec_hid_dim, dropout).to(device)\n",
        "decoder = Decoder(zh_vocab_size, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention).to(device)\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "z = model(x,y)\n",
        "print(z.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([87, 6, 16136]) torch.Size([6, 88])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36Rk-ov7Jczx",
        "outputId": "6e2d0067-5c0c-4df8-f7b3-6b6b14894e54"
      },
      "source": [
        "print(zh_vocab_size, en_vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16136 11356\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVmrMJczTbEt"
      },
      "source": [
        "# 将预测结果以好观察的形式呈现\n",
        "def sentence(lis):\n",
        "  j = len(lis) - 1\n",
        "  while lis[j] == '<padding>' or lis[j] == '<EOS>':\n",
        "    j -= 1\n",
        "  return ''.join(lis[:j+1])\n",
        "def translate(model, tri, tsi):\n",
        "  tren,tsen = trid_en[tri].unsqueeze(0), tsid_en[tsi].unsqueeze(0)\n",
        "  trzh, tszh = trid_zh[tri].unsqueeze(0), tsid_zh[tsi].unsqueeze(0)\n",
        "  tro, tso = model(tren, trzh, 0), model(tsen, tsen, 0)\n",
        "  tro1, tso1 = tro.squeeze(1).argmax(1), tso.squeeze(1).argmax(1)\n",
        "  tro2, tso2 = [zh_id2char[i] for i in tro1], [zh_id2char[i] for i in tso1]\n",
        "  print('train sentence:', sentence(tro2))\n",
        "  print('test sentence:', sentence(tso2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wbpL85i7lq4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "179982ba-ab59-4e6f-d687-e8b5003bfb91"
      },
      "source": [
        "# 开始训练\n",
        "num_epochs = 20\n",
        "batch_size = 128\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# 注，这里emb_dim编码器解码器共用\n",
        "emb_dim = 256\n",
        "enc_hid_dim = 512\n",
        "dec_hid_dim = 512\n",
        "dropout = 0.5\n",
        "attention = Attention(enc_hid_dim, dec_hid_dim)\n",
        "encoder = Encoder(en_vocab_size, emb_dim, enc_hid_dim, dec_hid_dim, dropout).to(device)\n",
        "decoder = Decoder(zh_vocab_size, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention).to(device)\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "tri, tsi = 20, 20\n",
        "print('correct train',sentence(tr_zh[tri]))\n",
        "print('correct test', sentence(ts_zh[tsi]))\n",
        "for epoch in range(num_epochs):\n",
        "  start, loss_sum, n = time.time(), 0.0, 0\n",
        "  dataset = torch.utils.data.TensorDataset(trid_en, trid_zh)\n",
        "  train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)\n",
        "  for tren, trzh in train_iter:\n",
        "    outputs = model(tren, trzh)\n",
        "    ov = outputs.view(-1, outputs.shape[-1])\n",
        "    # 因为outputs不包括起始字符，所以y也要去头\n",
        "    y = trzh[:, 1:]\n",
        "    yt = y.transpose(0, 1).contiguous().view(-1)\n",
        "    l = loss(ov, yt.long())\n",
        "    # 训练三部曲\n",
        "    optimizer.zero_grad()\n",
        "    l.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_sum += l.item() * trzh.shape[0]\n",
        "    n += trzh.shape[0]\n",
        "  try:\n",
        "    perplexity = math.exp(loss_sum / n)\n",
        "  except OverflowError:\n",
        "    perplexity = float('inf')\n",
        "  print(\"epoch:{} perplexity:{} time:{}\".format(epoch, perplexity, time.time() - start))\n",
        "  translate(model, tri, tsi)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "correct train <SOS>高雄市有一间音乐pub发生了枪击案5名男子和被害人有恩怨今天凌晨进入这间pub之后开枪示警嫌犯还拿起了酒罐和椅子殴打了被害人之后离去\n",
            "correct test <SOS>香港足球总会主席康保局介绍说这两名球员分别是快译通队的后卫冯希志和门将肖国基\n",
            "epoch:0 perplexity:14.675365485626488 time:62.239134550094604\n",
            "train sentence: 他的在的的的的的的的的的的的\n",
            "test sentence: 的的的的的的的的\n",
            "epoch:1 perplexity:8.177351049292026 time:61.207768201828\n",
            "train sentence: 新华社的的的的的的的的的的的的的的的的的\n",
            "test sentence: 在是的的的的的的的的\n",
            "epoch:2 perplexity:7.509777240235295 time:61.540637254714966\n",
            "train sentence: 新华社的的在的的的的的的的的的的的的的的的的的的的\n",
            "test sentence: 在在的的的的的的的的的的的的的\n",
            "epoch:3 perplexity:7.0865406215482345 time:61.851070165634155\n",
            "train sentence: 新华社北京１０月的的的的的的的的的的的的的的的的的的的\n",
            "test sentence: 在的的的的的的的的的的的的\n",
            "epoch:4 perplexity:6.7612953763845915 time:61.857831716537476\n",
            "train sentence: 新华社在在和中国的的的的的和的的和和的的的的的的和的的的\n",
            "test sentence: 在的的的的的的的的的的的\n",
            "epoch:5 perplexity:6.283706426377945 time:61.818536043167114\n",
            "train sentence: 新华社北京１１月电记者马晓霖的和和和和和和和和和和和和和和和和和和和和和和和和和和和和和\n",
            "test sentence: 在的的的的的的的和和中国的的\n",
            "epoch:6 perplexity:5.87985388254682 time:61.821486949920654\n",
            "train sentence: 在一名男子的的的一名男子和和的的的一名男子一名男子的的的的和和和和和和的的的\n",
            "test sentence: 在中国的的的的的的的的的的的的的的的的\n",
            "epoch:7 perplexity:5.492362190320075 time:61.8690972328186\n",
            "train sentence: 在一名男子的的的的的一个小时的的的一个的的的的一的一条的的一一一名男子的的的\n",
            "test sentence: 在的的的的的的的的的的的的的的的的的的\n",
            "epoch:8 perplexity:5.123270681807192 time:61.913825273513794\n",
            "train sentence: 在一名男子一名男子一名男子一名男子一名男子的一名男子一名男子和一名船员的一名男子一名船员的的一名理发师的\n",
            "test sentence: 由民主党的的的的的的的的和的的的的的的\n",
            "epoch:9 perplexity:4.5278200664406745 time:61.94269156455994\n",
            "train sentence: 高雄市一名男子一名男子一名嫌犯的的一名嫌犯了一名嫌犯的一名嫌犯的酒罐一名嫌犯了一名嫌犯了一名嫌犯了一名\n",
            "test sentence: 在美国的的的的的的的的的的的和的的的\n",
            "epoch:10 perplexity:4.1482922364001125 time:61.95156645774841\n",
            "train sentence: 高雄市一一名男子凌晨pub的的一名男子一名男子凌晨在pubpubpubpubpubpub的酒罐拿起pub的酒罐拿起pub一一个离去\n",
            "test sentence: 代表团的的的的的的的的的的的的的的的和的的\n",
            "epoch:11 perplexity:3.775180159591866 time:61.97601890563965\n",
            "train sentence: 高雄市一一名警官一名男子pub的枪击案的枪击案的的的示的示警凌晨拿起的酒罐拿起拿起拿起酒罐酒罐酒罐拿起的酒罐酒罐的酒罐酒罐酒罐的酒罐拿起\n",
            "test sentence: 的的的的的的的的的的和的的的的的的的的\n",
            "epoch:12 perplexity:3.3693051704761667 time:61.97850203514099\n",
            "train sentence: 高雄市一一一名嫌犯上午一名男子凌晨pub的一一一名嫌犯的一深一条pub一个离去了一一条的椅子椅子的椅子椅子的椅子一阵\n",
            "test sentence: 自在的的的的的的的的的的的的的的的及港龙大厦及港龙大厦\n",
            "epoch:13 perplexity:3.1043191325413506 time:61.946072816848755\n",
            "train sentence: 高雄市一名枪击案一名少年名枪击案一名男子pubpub一pub一名嫌犯pubpubpubpubpub的pub酒罐拿起pub酒罐拿起的椅子酒罐离去椅子的酒罐酒罐离去了酒罐\n",
            "test sentence: 在的的的的的的的的的的的的的的的的风的\n",
            "epoch:14 perplexity:2.973154655582739 time:62.038865089416504\n",
            "train sentence: 高雄市一一pub一pubpubpubpubpubpubpubpubpubpubpubpubpubpub被害人被害人被害人示pub之后离去了被害人的被害人被害人被害人了被害人\n",
            "test sentence: 西班牙司法的的的领导人沙龙的的的的公司在的的的的的的的\n",
            "epoch:15 perplexity:2.7630953819214352 time:61.98469662666321\n",
            "train sentence: 高雄市一高雄市高雄市高雄市高雄市的枪击案的少年被害人的一名男子凌晨被害人凌晨被害人的被害人被害人被害人被害人而且被害人被害人被害人被害人被害人的被害人椅子被害人被害人被害人的椅子椅子\n",
            "test sentence: 代表团西班牙西班牙的的组织者及其他及其他了两国的的的两个国家安全监察的\n",
            "epoch:16 perplexity:2.619337235821061 time:61.99689817428589\n",
            "train sentence: 高雄市威斯康辛高雄市九一座桥的一名嫌犯的一名嫌犯了被害人示警嫌犯的嫌犯被害人和被害人的酒罐酒罐酒罐酒罐酒罐酒罐酒罐酒罐酒罐酒罐酒罐离去了酒罐被害人\n",
            "test sentence: 美国广播电台的的争夺战的争夺战２１了美国的作文的的\n",
            "epoch:17 perplexity:2.514595197214474 time:62.002033710479736\n",
            "train sentence: 高雄市一高雄市高雄市pub一名少年了一的的的一男子凌晨的恩怨的示警嫌犯了被害人的pub之后的拿起拿起拿起了被害人被害人的椅子殴打了一\n",
            "test sentence: 在的的的的的的的的的的的的的的的的的的在中后中被的\n",
            "epoch:18 perplexity:2.37012263223163 time:61.98408508300781\n",
            "train sentence: 高雄市在高雄市一枪击案的音乐pub枪击案的枪击案被害人之后被害人之后开枪示警凌晨被害人的被害人示警凌晨被害人的被害人示警嫌犯了被害人\n",
            "test sentence: 在计算的的的的的的了历年在国际的的的及假的了的包括在新加坡航空公司共同\n",
            "epoch:19 perplexity:2.342650745414561 time:61.9884991645813\n",
            "train sentence: 高雄市一名男子凌晨在音乐pub的一名嫌犯了被害人被害人的pub之后开枪示警凌晨被害人的嫌犯嫌犯开枪示警嫌犯了酒罐被害人\n",
            "test sentence: 的的的的的的的的的的颗及西班牙大部分缺乏进取的了西弗吉尼亚州的包括汉堡\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6L4WhmAAZJV3",
        "outputId": "c7c1db3b-49ec-41ee-8058-97ffe18ed965"
      },
      "source": [
        "def random_translate(model, n):\n",
        "  for i in range(n):\n",
        "    x = random.randint(0, len(ts_en) - 1)\n",
        "    print('correct train:',sentence(tr_zh[x]))\n",
        "    print('correct test:', sentence(ts_zh[x]))\n",
        "    translate(model, x, x)\n",
        "    print('\\n')\n",
        "random_translate(model, 5)\n",
        "\n",
        "# 可以看到，train的拟合效果勉强还行，test的效果就比较差了"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "correct train: <SOS>科什图尼察对美国之音的塞尔维亚语部表示反对派同所有的有关的国家机构建立了联系包括警方一个新的民主政府正在形成\n",
            "correct test: <SOS>著名学者戴逸先生说外国传教士在鸦片战争中跟着侵略军大批的涌进中国来其外国的侵略和传教事业从一开始就密切的联系在一起\n",
            "train sentence: 科什图尼察表示最近的塞尔维亚语塞尔维亚语塞尔维亚语的塞尔维亚语塞尔维亚语的反对派的一个新政府正在形成的一个新政府正在形成\n",
            "test sentence: 有表示这名的的的的在在美国和的的名巴以巴以巴以冲突中的的近4000的近\n",
            "\n",
            "\n",
            "correct train: <SOS>新公司的股票在昨天早盘稍微下跌到18.31英镑之后逐渐回升到发售价18.43英镑\n",
            "correct test: <SOS>而台北市警方上午就进行了一场防抢演习强势的警力希望枭枭不要轻举妄动\n",
            "train sentence: 新政府的下跌的的的下跌的18.31英镑新英镑英镑新英镑\n",
            "test sentence: 从海上的的的的一方并且声称的一个而的的一个小时\n",
            "\n",
            "\n",
            "correct train: <SOS>死伤人员家属情绪稳定\n",
            "correct test: <SOS>副总统律师团今天将前往台北地院去递状控告新新闻诽谤\n",
            "train sentence: 死伤人员伤亡情绪在情绪的情绪\n",
            "test sentence: 法官的将对俄对俄要求恢复的的将引发了\n",
            "\n",
            "\n",
            "correct train: <SOS>结果谁也无法预料\n",
            "correct test: <SOS>来自上海的黄闻涛在男子f12级三级跳远比赛中以14米16的好成绩打破残奥会记录获得金牌\n",
            "train sentence: 反正没看过\n",
            "test sentence: 和的的的的的的的的不足不足的的的的的的的的一批的的一批典型\n",
            "\n",
            "\n",
            "correct train: <SOS>搞卫生工作量也不小那也是太太的事\n",
            "correct test: <SOS>泰国警方在与逃犯谈判后允许他们带着人质乘坐1辆轻型卡车逃跑警方一直紧随其后最终顺利击毙逃犯结束了21小时的人质危机\n",
            "train sentence: 而的工作量也不可太太的太太\n",
            "test sentence: 从后后后在后后的后在一名男子的一名男子的一一个小时的一的一场斗争\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9GQy0dEZ7Wr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}